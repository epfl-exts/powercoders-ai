{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Detection\n",
    "\n",
    "<img src=\"https://cdn.pixabay.com/photo/2015/08/24/20/13/welcome-905562_1280.png\" width=\"500px\" align=\"left\" />\n",
    "<div style=\"clear: both\"></div>\n",
    "\n",
    "## Question?\n",
    "\n",
    "- How good are you in detecting different languages?\n",
    "- What do you think is your accuracy? How many mistakes would you make?\n",
    "\n",
    "And what about a machine? How well do you think would a machine do this task?\n",
    "\n",
    "Well, let's go ahead and find out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download data\n",
    "---\n",
    "\n",
    "First things first, we need a dataset to train our machine on. One of the quickest and most straight forward sources of text in different languages is Wikipedia.\n",
    "\n",
    "So let's go ahead and extract the main text from a wikipedia page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # Library to retrieve web pages\n",
    "from bs4 import BeautifulSoup # Library to parse them\n",
    "\n",
    "def get_text(url):\n",
    "    page = BeautifulSoup(requests.get(url).text, 'html.parser') # Get page\n",
    "    paragraphs = page.find_all('p') # .. paragraphs\n",
    "    return '\\n'.join([p.text for p in paragraphs]) # and texts!\n",
    "\n",
    "# Let's check the results\n",
    "get_text('http://github.com')[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! Let's get some data for each language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "pages = {\n",
    "    'Deutsch': 'https://de.wikipedia.org/wiki/Schweiz',\n",
    "    'Français': 'https://fr.wikipedia.org/wiki/Suisse',\n",
    "    'Itialiano': 'https://it.wikipedia.org/wiki/Svizzera',\n",
    "    'Rumantsch': 'https://rm.wikipedia.org/wiki/Svizra',\n",
    "    'English': 'https://en.wikipedia.org/wiki/Switzerland',\n",
    "    # Todo - add more languages from Wikipedia!\n",
    "}\n",
    "\n",
    "# Retrieve texts\n",
    "texts = { language: get_text(url) for language, url in pages.items() }\n",
    "languages = list(texts.keys())\n",
    "print(json.dumps(languages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode data for ML\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are features that distinguish well the different languages?\n",
    "\n",
    "* `{switzerland, suisse, schweiz, svizzera}` work well for those documents, but **don't generalize** well to new ones!\n",
    "* `{is, est, ist, è}` work well and generalize better, but are handpicked - what if we have 100 languages?\n",
    "* Also, not all languages have words separated by spaces!\n",
    "\n",
    "Idea, **char N-grams**: extract popular combinations of N characters for each language .. but how to choose N?\n",
    "\n",
    "* Small Ns generalize better, but are not necessarily relevant to distinguish between languages\n",
    "* Large Ns are more language-specific, but don't necessarily generalize well to new documents\n",
    "\n",
    "Using **3-grams** is a good tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# .. to extract char N-grams\n",
    "vectorizer = CountVectorizer(max_features=100, ngram_range=(3,3), analyzer='char')\n",
    "\n",
    "# .. create \"vocabulary\"\n",
    "vocabulary = []\n",
    "for language, text in texts.items():\n",
    "    ngrams = vectorizer.fit([text]).get_feature_names() # Extract popular N-grams\n",
    "    vocabulary.extend(ngrams) # Store them\n",
    "    \n",
    "# Remove potential duplicates\n",
    "vocabulary = list(set(vocabulary))\n",
    "print(json.dumps(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data\n",
    "---\n",
    "\n",
    "We are going to train our model on some data. Let's extract small sentences from our webpages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "# Extract random sentences\n",
    "X, y = [], []\n",
    "N = 10000 # Number of data points\n",
    "length = 30 # Length of sentences\n",
    "\n",
    "for _ in range(N):\n",
    "    # Random language\n",
    "    language = np.random.choice(languages)\n",
    "    y.append(languages.index(language))\n",
    "    \n",
    "    # Random text subset\n",
    "    idx = np.random.randint(0, len(texts[language]) - length)\n",
    "    sample_text = texts[language][idx:idx+length]\n",
    "    X.append(sample_text)\n",
    "    \n",
    "# Vectorize\n",
    "vectorizer = CountVectorizer(vocabulary=vocabulary, ngram_range=(3,3), analyzer='char')\n",
    "X_vectorized = vectorizer.transform(X)\n",
    "y_encoded = to_categorical(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model\n",
    "---\n",
    "\n",
    "Now we come to the fun part! Let's create a simple neural network and train it on the sentences that we just prepared!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create neural network\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(units=len(languages), activation='softmax', input_shape=[len(vocabulary)]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['acc']\n",
    ")\n",
    "\n",
    "# Trick: end training when accuracy stops improving (optional)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=2)\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    x=X_vectorized, y=y_encoded, batch_size=32, epochs=20, # max \"epochs\"\n",
    "    validation_split=0.7, callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot results\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "\n",
    "# Plot loss values\n",
    "ax1.set_title('loss: {:.4f}'.format(history.history['val_loss'][-1]))\n",
    "ax1.plot(history.history['val_loss'], label='validation')\n",
    "ax1.plot(history.history['loss'], label='training')\n",
    "ax1.legend()\n",
    "\n",
    "# plot accuracy values\n",
    "ax2.set_title('accuracy: {:.2f}%'.format(history.history['val_acc'][-1]*100))\n",
    "ax2.plot(history.history['val_acc'], label='validation')\n",
    "ax2.plot(history.history['acc'], label='training')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model\n",
    "---\n",
    "\n",
    "Finally, let's test our model on new sentences!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sample_texts = [\n",
    "    'il n’y a pas le feu au lac',\n",
    "    'der april macht was er will',\n",
    "    'success is a team sport'\n",
    "]\n",
    "preds = model.predict(vectorizer.transform(sample_texts))\n",
    "pd.DataFrame(preds, index=sample_texts, columns=languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the model for the web!\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "tf.keras.models.save_model(model, 'classifier.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running the notebook via [Google Colab](https://colab.research.google.com/), run\n",
    "\n",
    "```bash\n",
    "!pip install tensorflowjs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Prepare model for TensorFlow.js\n",
    "!tensorflowjs_converter --input_format keras 'classifier.h5' 'tfjs-model'\n",
    "\n",
    "# Zip the result!\n",
    "shutil.make_archive('tfjs-model', 'zip', 'tfjs-model');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
